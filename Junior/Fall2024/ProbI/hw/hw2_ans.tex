\documentclass[a4paper]{article}
\usepackage[a4paper, margin=1in]{geometry}
\input{preamble}
\title{\Huge{Probability I Hw 2}}
\author{\huge{Daniel Yu}}
\date{September 23, 2024}

\pdfsuppresswarningpagegroup=1

\begin{document}
\maketitle
\newpage% or \cleardoublepage
% \pdfbookmark[<level>]{<title>}{<dest>}
\pagebreak
\begin{enumerate}
  \item Are the random variables $T_n,P_n,S_n$ independent? 
    \begin{proof}{Proof by contradiction}\\
      Let $\Omega = \{T,P,S\}^n$ the possible combinations of tops, pants, and shoes where $P(T) = \frac{1}{2}, P(P)=\frac{1}{3}, 
      P(S) = \frac{1}{6}$ for each week. Then the random variables $T_n, P_n, S_n : \Omega \to \{1,2,3,\ldots,n\} \subseteq \R $ represent the number of tops, 
      pants, and shoes respectively that were bought after $n$ weeks where each week is independent of the previous 
      weeks. Thus, $T_n, P_n, S_n \sim Binomial(n,p)$. \\


      For $T_n, P_n, S_n$ to be independent random variables, they must be pairwise independent and 
      jointly independent i.e.  $P(T_n = a \cap P_n = b \cap S_n = c) = P(T_n=a) \cdot P(P_n=b) \cdot P(S_n=c)$. We 
      know that $P(T_n=a) = \begin{pmatrix} n \\ a \end{pmatrix} \left( \frac{1}{2} \right)^a \left( \frac{1}{2} \right)^{n-a}
      = \begin{pmatrix} n \\ a \end{pmatrix} \left( \frac{1}{2} \right)^n$, 
      $P(P_n = b) =  \begin{pmatrix} n \\ b \end{pmatrix} \left( \frac{1}{3} \right)^b \left( \frac{2}{3} \right)^{n-b} $, $P(S_n=c) =
       \begin{pmatrix} n \\ c \end{pmatrix} \left( \frac{1}{6} \right)^c \left( \frac{5}{6} \right)^{n-c} $. However, $P(T_n = a \cap P_n = b \cap S_n = c)
       = \begin{pmatrix} n\\ a\end{pmatrix} (\frac{1}{2})^a \cdot \begin{pmatrix} n-a \\ b \end{pmatrix} (\frac{1}{3})^b
       \cdot \begin{pmatrix} n - b - a \\ c \end{pmatrix} \frac{1}{6}^{c}$ with the given constraint that $c =  n - a -b$ else the probability is  $0$. 
       Clearly, the left hand side and right hand side are not the same. \\ 

       For example, consider the $P(T_n = n)$ i.e. when all the items chosen after $n$ weeks are tops,  $P(T_n=n) = 
       \text{ no. ways} \cdot \text{ probability} = 1 \cdot \frac{1}{2}^n$. Then $P(T_n = n \cap P_n = b \cap S_n = c) = 0$
       when $b,c > 0$. However, suppose $b,c=1$,  $ P(T_n=n) \cdot P(P_n=1) \cdot P(S_n=1) = \frac{1}{2}^n \cdot 
       \begin{pmatrix} n\\1 \end{pmatrix} \frac{1}{3}\frac{1}{3}^{n-1} \cdot \begin{pmatrix} n \\ 1 \end{pmatrix} \frac{1}{6} \frac{5}{6}^{n-1} 
       \neq 0$! Thus, the three random variables are not independent!
    \end{proof}
  \item In the same setup as problem 1, compute $E[T_n - P_n]$ and  $Var(T_n - P_n)$.
    \begin{proof}
      The expected value is additive, so $E[T_n - P_n] = E[T_n] - E[P_n]$. We know  $E[T_n] = \frac{n}{2}$ and
      $E[P_n] = \frac{n}{3}$, so $E[T_n - P_n] = \frac{n}{2} - \frac{n}{3} = \frac{n}{6}$. \\

      To compute $Var(T_n - P_n)$, let
       \begin{align*}
         Var(T_n - P_n) &= E[(T_n - P_n)^2] - E[T_n - P_n]^2 
      .\end{align*}
      Consider the random variable $D_n = T_n - P_n$. 
      We could attempt to map out the probabilities for each outcome for the random variable $D_n$ and $D_n^2$ but that
      gets extremely complicated. Instead, recognize that each week's outcome is independent from each other
      weeks outcome by definition, so we can break down the cumulative difference of $T_i - P_i$ for $i = 1,2,\ldots,n$ 
      weeks as the sum of the individual differences each week, i.e. $D_n = d_1 + d_2 + \ldots + d_n$ where each
      $d_i$ is the random variable representing the difference between tops and pants for that week (also recall
      that selecting one item is mutually exclusive with selecting all other items), so 
      $d_i \in \{-1,0,1\}$ where $d_i=1$ means that there was a top selected and $d_i = 0$ means that there was 
      a hat selected and  $d_i=-1$ menas that there was a pants selected. Although variance is nonlinear in general
      , since each $d_i$ is independent, $Var(D_n) = Var(d_1 + d_2 + \ldots + d_n) = Var(d_1) + Var(d_2) + 
      \ldots + Var(d_n)$ as independent variables $X,Y$ have  $Cov(X,Y) = 0$ (independence > correlation). This 
      gives:
      \begin{align*}
        Var(T_n - P_n) = Var(D_n) &= Var(d_1) + Var(d_2) + \ldots + Var(d_n) \\ 
                                  &= \sum_{i=1}^n Var(d_i) \\
                                  &= nVar(d_1) \text{ since each $d_i$ has the same distribution} \\
                                  &= n (E[d_1^2] + E[d_1]^2) 
      .\end{align*}
      The distribution of $d_1$ is  $P(d_1 = 1) = \frac{1}{2}$, $P(d_1 = 0) = \frac{1}{6}$, $P(d_1 = -1) = \frac{1}{3}$. 
      So,
      \[
        E[d_1]^2 = \left( 1 \cdot \frac{1}{2} + 0 \cdot \frac{1}{6} + -1 \cdot \frac{1}{3}\right)^2 = \frac{1}{6}^2 = \frac{1}{36} 
      .\]
      and the distribution of $d_1^2$ is  $P(d_1^2 = 1) = P(d_1=1) + P(d_1=-1) = \frac{1}{2} + \frac{1}{3} = \frac{5}{6}$ and 
      $P(d_1^2 = 0) = P(d_1=0) = \frac{1}{6}$
      \[
        E[d_1^2] = \frac{5}{6} + 0 = \frac{5}{6} 
      .\]
      The answer is then:
      \begin{align*}
        n (E[d_1^2] + E[d_1]^2) &= n (\frac{5}{6} - \frac{1}{36})\\
                                &= n(\frac{29}{36})
      .\end{align*}
    \end{proof}
  \item Show that $P[R \geq 11] \leq \frac{1}{2}$.
    \begin{proof}
      We toss a fair coin 1000 times, so $\Omega = \{H,T\}^{1000}$ where each toss is independent of all others. 
      R is the random variable representing the longest consecutive run of 'Heads'. The $P(R \geq 11) = P(R = 11 \cup
      R=12 \cup \ldots \cup R=1000) = P(R=11) + P(R=12) + \ldots + P(R=1000) = \sum_{i=11}^{1000} P(R=i)$. However,
      it is really hard to compute the probability of $P(R=i)$ explicitly. Instead, what we can do is break 
      $P[R\geq 11]$ into a union of not necessarily disjoint events and compute their sums to provide an
      upper bound on the probability of $P[R \geq 11]$. Begin our decomposition as follows:
      \begin{align*}
        P[R \geq 11] &= P[\{\text{coins 1 to 11 are heads} \cup \text{coins 2 to 12 are heads} \cup \ldots \cup \text{coins 990 to 1000 are heads} \} \\ 
                     &\leq P[\text{coins 1 to 11 are heads}] + P[\text{coins 2 to 12 are heads}] + \ldots + P[\text{coins 990 to 1000 are heads}] 
      .\end{align*}
              
      Notice that each of the events are not disjoint. For example, consider the event when coins 1 to 13 are heads, this
      would be counted for coins 1 to 11 are heads and coins 2 to 12 are heads. However, the probabilities of these
      events are now much easier to compute. Each $\forall i = 1,\ldots,990$:
       \[
      P(C_i) = \frac{1}{2}^{11}
      .\] 
      Then, we know :
      \begin{align*}
        P[R \geq 11] &\leq \sum_{i=1}^{990}\frac{1}{2}^{11} \\
                     &\leq 990 * \frac{1}{2}^{11} \\
                     &\leq .483\ldots \\
                     &\leq .5
       .\end{align*}
     \end{proof}
  \item Party that fits 20 people, 24 people are invited
    \begin{enumerate}
      \item What is the expected number of people who will attend and the probability that all attendees fit
        inside the venue if the probability is $\frac{5}{6}$ that any one person shows up? 
        \begin{note}
          Since, each person is indpendent of each other and they each have $\frac{5}{6}$ probability to show up.
          The number of people who show up $X \sim Binomial(24, \frac{5}{6})$, so $E[X] = \frac{5}{6} \cdot 24 = 20$. \\

          The probability that all attendes will fit inside the venue is $P[X \leq 20] = 1 - P[X \geq 21] = 1 - 
          (\begin{pmatrix} 24 \\ 21 \end{pmatrix} \frac{5}{6}^{21} \frac{1}{6}^{3} +  \begin{pmatrix} 24 \\ 22 \end{pmatrix} \frac{5}{6}^{22} \frac{1}{6}^{2}
          +  \begin{pmatrix} 24 \\ 23 \end{pmatrix} \frac{5}{6}^{23} \frac{1}{6}^{1} + \frac{5}{6}^{24}) \approx 0.584$ 
        \end{note}
      \item Suppose now that $24$ people come in groups of 4 with probability $\frac{5}{6}$, what is the expected
        number of people that will arrive and the probability that all attendees fit in the venue.
        \begin{note}
          There are now 6 groups of people with 4 people each and each group of people has $\frac{5}{6}$ chance
          to come. Now, $X$ represents the number of people and not groups. We know that $\Omega = \{\left( 0,0,0,0,0,0 \right),
          \left( 0,1,0,0,0,0 \right), \ldots, \left( 1,1,1,1,1,1 \right) \}$, the attendance pattern of groups coming, 
          and $X$ maps from  $\Omega$ to  $\R$, so  $X = \{0,4,8,12,16,20,24\}$. If we let $Y$ be the random variabl e
          denoting the number of groups that came. Then $E[X] = E[4 Y] = 4 E[Y] = 4 \left( \frac{5}{6}  \cdot 6 \right) = 20$
          The probability that all the attendees fit inside the venue is now $P[X \leq 20] = P[Y \leq 5] = \sum_{k=1}^5 
          \begin{pmatrix} 6 \\ k \end{pmatrix}\frac{5}{6}^{k} \cdot \frac{1}{6}^{6-k} \approx 0.665$ 
        \end{note}
    \end{enumerate}
  \item Suppose we have $r$ balls to be distributed among  $n$ bins. Each of  $n^{r}$ configurations are equally 
    likely. For any $k \in \{1,2,\ldots,n\}$, calculate the probability first $k$ bins are empty.
    \begin{proof}
      Let $K$ be the random variable denotating the first $k$ bins that are empty. If the configurations 
      are equally likely, then $P[K=k] = \frac{\text{ no configurations with first k bins empty }}{\text{ total configurations}}
      P[K = k] = \frac{(n-k)^{r}}{n^r} = \frac{n-k}{n}^{r}$
    \end{proof}
    Question: Doesn't stars and bars apply?
      Let $K$ be the random variable denotating the first $k$ bins that are empty. If the configurations 
      are equally likely, then $P[K=k] = \frac{\text{ no configurations with first k bins empty }}{\text{ total configurations}}
      = \frac{\begin{pmatrix} n + r - k - 1 \\ r \end{pmatrix} }{ \begin{pmatrix} n + r - 1 \\ r \end{pmatrix} }$
\end{enumerate}
\end{document}
