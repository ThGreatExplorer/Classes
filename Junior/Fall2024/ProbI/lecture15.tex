\documentclass[a4paper]{article}
\usepackage[a4paper, margin=1in]{geometry}
\input{preamble}
\title{\Huge{Probability 1}}
\author{\huge{Daniel Yu}}
\date{November 19, 2024}

\pdfsuppresswarningpagegroup=1

\begin{document}
\maketitle
\newpage% or \cleardoublepage
% \pdfbookmark[<level>]{<title>}{<dest>}
\tableofcontents
\pagebreak
\section{Poisson Processes}

\begin{definition}
  A stochastic process $\{N(t)\}_{t \in [0, \infty)} $ is a counting process if
\begin{enumerate}
  \item $N(t) = \{01,2,3,\ldots\} \forall t > 0 $ and
  \item $N(s) \leq N(t) \forall s \leq t$
\end{enumerate}
$N(t)$ counts the number of events that occured between time  $0$ and time  $t$.
\end{definition}


\begin{note}
  See the Appendix for the definition for asympotic notation for $O(x)$. We are using a definition that is no the traditional definition. 
\end{note}
\begin{definition}

A counting process is \textbf{simple} if $\forall t \geq 0,$
 \[
   P[N(t + h) - N(t) \geq 2] = O(h)
.\] 
meaning the probability of more than 1 arrival in the interval $[t+h, t]$ goes to 0 fairly quickly (at rate $O(h)$ )
  
\end{definition}

\noindent\hrulefill

We wish to study the generalization of \textbf{time-homogenous markov chains}. Let's make the assumptions that:
\begin{enumerate}
  \item \textbf{the distribution of (N(t+s) - N(t)) the number of arrivals that occured between t to t+s does not depend on t!}
  \item \textbf{for any $t \leq s$ the RVs N(t), N(s) - N(t) are independent} this is known as the independent increament property. This is saying that the number of arrivials in disjoint time intervals is independent. 
\end{enumerate}

\begin{definition}
  Any simple counting process that satisfies (1), (2) is a time-homogenous poisson process on $\R$
\end{definition}

To determine the process, we need only one parameter $\lambda > 0$. Consider for the interval, $[0,h]$:
 \[
   P[N(h) - N(0) = 0] = 1 - \lambda h + O(h)
.\] 
using the fact that $P[N(h) - N(0) = 1] = \lambda h $ and  $P[N(h) - N(0) \geq 2] = O(h)$

\begin{theorem}
  Let $N_{\lambda}(t)$ be the unique process which satisfies all these properties above:
  \[
    P[N_{\lambda} (t) = k] = \frac{e^{-\lambda t} (\lambda t)^{k}}{k!}
  .\] 
  i.e. $N_{\lambda}(t) \sim Poisson(\lambda t)$

\end{theorem}
  \noindent\hrulefill

  \begin{proof}
    Let $P_m (t) = P[N_{\lambda}(t) = m]$. 
    \begin{align*}
      P_m (t+ h) &= \sum_{k = 0}^{m} P[N_{\lambda} (1+ h) = m \mid N_{\lambda}(t) = k] \cdot P[N_{\lambda} (t) = k] \\
                 &= \sum_{k = 0}^{m} P[N_{\lambda} (1+ h) - N_{\lambda} (t) + N_{\lambda} (t) = m \mid N_{\lambda}(t) = k] \cdot P[N_{\lambda} (t) = k]   \\
                 &= \sum_{k = 0}^{m} P[N_{\lambda} (1+ h) - N_{\lambda } (t) = m  - k \mid N_{\lambda}(t) = k] \cdot P[N_{\lambda} (t) = k]  \\
                 &\text{ Use the property that disjoint intervals are independent}  \\
                 &= \sum_{k = 0}^{m} P[N_{\lambda} (1+ h) - N_{\lambda } (t) = m  - k] \cdot P[N_{\lambda} (t) = k]   \\
                 &= \text{using time homogenity} \\
                 &=  \sum_{k=0}^{m} P[N_{\lambda} (h) - N_{0} (t) = m  - k] \cdot P[N_{\lambda} (t) = k]   \\
                 &=  P[N_{\lambda}(h) - N_{\lambda}(0) = 0 ] \cdot P_{m}(t) + P[N_{\lambda}(h) - N_{\lambda}(0) = 0 ] \cdot P_{m-1}(t) + \\
                 &\sum_{k=0}^{m-2} P[N_{\lambda} (h) - N_{0} (t) = m  - k] \cdot P_k (t)  \\
                 &= (1- \lambda h + O(h)) P_{m} (t) + (\lambda h + O(h)) P_{m-1} (t)  + \sum_{k=0}^{m-2} O(h) \cdot P_{k} (t) \\
                 &\text{ we can group and simplify by asymptotics since $cO(x) = O(x)$ } \\
                 &= P_{m}(t) + \lambda h \left(   -P_{m}(t) + P_{m}(t) \right) + O(h) \\
     \frac{P_m (t + h) - P_m (t)}{h} &= \lambda (- P_{m} (t) + P_m (t)) + \frac{O(h)}{h}  
    .\end{align*}
    Let's take the limit as $h \to 0$:
     \[
    \frac{d P_{m} (t)}{dt} = - \lambda P_{m} (t) + \lambda P_{m} (t), m \geq 1
    .\] 
    Repeating this for $m = 0$,m 
     \[
    \frac{d P_0 (t)}{dt} = - \lambda P_0 (t)
    .\]
    What is $P_0 (t)$?
     \begin{align*}
       P_0' &= -\lambda P_0 \\
      \Rightarrow \int_{0}^{t} \frac{P_{0}' (s)}{P_{0} (s)} ds \\
            &= \int_{0}^{t}  -\lambda ds \\
            &=  \ln(P_0(t)) - \ln(P_0(0)) = - \lambda t \\
       P_0(t) &= e^{-\lambda t} 
    .\end{align*}
    For $m=1$:
    missing notes.\\

     $e^{\lambda t} P_{1} (t) - P_{1} \left(0 \right)  = \lambda t \Rightarrow P_1 (t) = \lambda t \cdot e^{ -\lambda t}$ \\
     It is possible to solve the above by induction and get the closed form formula:
     \[
       P_{m}(t) \frac{e^{-\lambda t} (\lambda t)^{m}}{m!}
     .\] 

     \noindent\hrulefill

     Method 2: Using Generating Functions\\
     \[
     G(s,t) = \sum_{m=0}^{\infty} P_{m} (t) \cdot S^{m}
     .\] 
     Then, compute the derivative 
     \begin{align*}
       \frac{d G(s,t)}{dt} &= \sum_{m=0}^{\infty} P_m (t)' S^{m} \\
                           &= P_0'(t) + \sum_{m \geq 1} P_{m}'(t) S^{m}  \\
                           &= - \lambda P_{0}(t) + \sum_{m \geq 1} [ - \lambda P_{m} (t) + \lambda P_{m+1} (t)] S^{m} \\ 
                           &= \text{missing text}  \\
                           &= \lambda G(s,t) + \lambda s G(s,t) \\
     .\end{align*}
     Then,
     \[
     \frac{d G(s,t)}{dt} = \lambda G(s,t) (S-1)
     .\] 
     Divide by $G(s,t)$ and integrat w.r.t.  $t$,
      \begin{align*}
        \ln[G(s,t)] - \ln[G(s,t)] = \lambda s-1) \cdot t
     .\end{align*}

     missing text \\

     \begin{align*}
       \sum_{m = 0}^{\infty} P_m (t) \cdot S^{m} &= e^{-\lambda t} \cdot e^{\lambda s t} \\
       &= e^{-\lambda t} \sum_{m=0}^{\infty} \frac{\left( \lambda s t \right)^{m} }{m!} \\
      &\text{canceling both sides and removing the summations} \\
      P_m (t) &= \frac{e^{-\lambda t} (\lambda t)^{m}}{m!}  
     .\end{align*}
  \end{proof}

\begin{note}
  The continuous version of this is the Brownian motion
\end{note}

\noindent\hrulefill

Let's go through an example. Let $N(t)$ be a poisson point process of increment 1. What is
 \[
   P[N(1) = 1, N(2) = 1, N(3) = 2, N(4) = 2, N(5) = 3] 
.\]?\\

\textbf{Method 1}\\
We can use the new arrivals per interval:
\[
  = P[\text{1 arrival [0,1], 0 arrival in [1,2], one arrival in [2,3], zero arrivasl [3,4], 1 arrova; [4,5]}]
.\] 

By tyhe time independent increment property:
\begin{align*}
  P[\text{ events above}] &= P[\text{1 arrival [0,1]}]P[\text{0 arrival [1,2]}] P[\text{1 arrival [2,3]} P[\text{0 arrival [3,4]}] P[\text{1 arrival [4,5]}] \\
                          &= P[Poisson(1) = 1]^{3} P[Poisson(1) = 0]^{2} \\
                          &= \frac{1}{e^{5}} \\
.\end{align*}

\textbf{Method 2}\\
Using joints intelligently
\begin{align*}
  P[N(1) = 1, N(2) = 1, N(3) = 2, N(4) = 2, N(5) = 3] &=
P[N(1) - N(0) = 1] \cdot P[N(2) - N(1) = 0] \cdot P[N(3) - N(2) = 1] \\
&= \cdot P[N(4) - N(3) = 0] \cdot P[N(5) - N(4) = 1]. \\
.\end{align*}
\[
P[X = k] = \frac{\mu^k e^{-\mu}}{k!}.
\]
For each interval $[a, b]$, the mean is $\mu = \lambda (b-a)$. Since $\lambda = 1$ and each interval is of length 1, we have $\mu = 1$ for each increment:
\begin{align*}
  P[N(1) - N(0) = 1] &= P[\text{Poisson}(1) = 1] = \frac{1^1 e^{-1}}{1!} = \frac{1}{e}, \\
  P[N(2) - N(1) = 0] &= P[\text{Poisson}(1) = 0] = \frac{1^0 e^{-1}}{0!} = \frac{1}{e}, \\
  P[N(3) - N(2) = 1] &= P[\text{Poisson}(1) = 1] = \frac{1}{e}, \\
  P[N(4) - N(3) = 0] &= P[\text{Poisson}(1) = 0] = \frac{1}{e}, \\
  P[N(5) - N(4) = 1] &= P[\text{Poisson}(1) = 1] = \frac{1}{e}.
\end{align*}

\textbf{Step 4: Combine Probabilities}\\
The total joint probability is:
\[
P[N(1) = 1, N(2) = 1, N(3) = 2, N(4) = 2, N(5) = 3] =
\frac{1}{e} \cdot \frac{1}{e} \cdot \frac{1}{e} \cdot \frac{1}{e} \cdot \frac{1}{e} = \frac{1}{e^5}.
\]

\textbf{Final Result:}\\
Both methods yield the same result:
\[
P[N(1) = 1, N(2) = 1, N(3) = 2, N(4) = 2, N(5) = 3] = \frac{1}{e^5}.
\]
\section{Appendix: Asysmpotics}
\begin{definition}
  $f(x) = O(x)$ if  $\lim_{x \to 0} \frac{f(x)}{x} = 0$

  \begin{note}
    This not equivalent to the usual definition of big-O notation i.e. that $f(x)$ is  $O(g(x))$ if $f(x) \leq cg(x)$. 
    \begin{enumerate}
      \item consider $x^{2} \Rightarrow \lim_{x \to 0} \frac{x^{2}}{x} = \lim_{x \to 0} x = 0$ but is not $O(x)$ in the traditional definition.
      \item Consider $x \Rightarrow \lim_{x \to 0} \frac{x}{x} = 1 \neq 0$ but is $O(x)$ in the traditional definion.
    \end{enumerate}  
  \end{note}
\end{definition}

\noindent\hrulefill 

In this notation if $f(x)$ is twice differentiable at $x=0$, then we can use the taylor series expansion with the error term $r(x) = O(x^{2})$:
 \[
f(x) = f(0) + f'(0) + \frac{f''(0)}{2} x^{2} + O(x^{2})
.\] 

\[
  (\frac{1}{x} + 1 + O(1)) \cdot \left( 1 + O(x) \right)  = \left( \frac{2}{x} + 2 \right) + O(1) 
.\] 

some missing text here
$\frac{f(x)}{x} + f(x) \to 0$

\end{document} 
